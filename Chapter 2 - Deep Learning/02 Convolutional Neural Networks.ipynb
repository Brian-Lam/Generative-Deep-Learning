{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9e2e7e-a1a4-412b-af03-19cbaf52f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 29ms/step - accuracy: 0.3886 - loss: 1.8057\n",
      "Epoch 2/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.5798 - loss: 1.1898\n",
      "Epoch 3/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.6475 - loss: 1.0167\n",
      "Epoch 4/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.6758 - loss: 0.9227\n",
      "Epoch 5/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7052 - loss: 0.8445\n",
      "Epoch 6/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-30s\u001b[0m -18998us/step - accuracy: 0.7218 - loss: 0.7934\n",
      "Epoch 7/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7389 - loss: 0.7530  \n",
      "Epoch 8/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7493 - loss: 0.7136   \n",
      "Epoch 9/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7648 - loss: 0.6757 \n",
      "Epoch 10/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7777 - loss: 0.6413  \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7168 - loss: 0.8302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8373697400093079, 0.7171000242233276]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, utils, layers, models, optimizers\n",
    "\n",
    "\n",
    "# Load the CIFAR-10 dataset. This dataset is a collection of 60,000 32x32 color images\n",
    "# in 10 different classes (e.g., airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).\n",
    "# It's automatically split into training and testing sets.\n",
    "# (x_train, y_train): This contains the training images (x_train) and their corresponding labels (y_train).\n",
    "# (x_test, y_test): This contains the testing images (x_test) and their corresponding labels (y_test).\n",
    "# The images are initially represented as pixel intensity values ranging from 0 to 255.\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Perform Data Pre-processing.\n",
    "#\n",
    "# Normalize the RGB pixel values.\n",
    "# Images are typically represented with pixel values ranging from 0 (black) to 255 (white or full color intensity).\n",
    "# Neural networks generally perform better and train faster when input features are scaled\n",
    "# to a smaller, consistent range, such as 0 to 1.\n",
    "# By dividing by 255.0 (using a float to ensure float division), we transform each pixel value\n",
    "# from its original 0-255 range to a new range between 0.0 and 1.0.\n",
    "# `.astype('float32')`: We also explicitly convert the data type to float32.\n",
    "# This is a common practice in deep learning as models often operate with floating-point numbers\n",
    "# for calculations, and float32 provides a good balance between precision and memory usage.\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Create one-hot encodings for the labels.\n",
    "# Currently, our `y_train` and `y_test` labels are integers (e.g., 0, 1, 2, ..., 9),\n",
    "# where each integer represents a specific class.\n",
    "# For multi-class classification problems with a Softmax output layer (which is typical),\n",
    "# neural networks expect the labels to be in a \"one-hot encoded\" format.\n",
    "#\n",
    "# In one-hot encoding, we have single integer label in a vector where only one element is \"hot\" (1)\n",
    "# and all others are \"cold\" (0). The position of the '1' corresponds to the class.\n",
    "#\n",
    "# Example with NUM_CLASSES = 10:\n",
    "# - If y_label is 0 (e.g., 'airplane'), it becomes [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]\n",
    "# - If y_label is 1 (e.g., 'automobile'), it becomes [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]\n",
    "# - If y_label is 9 (e.g., 'truck'), it becomes [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]\n",
    "#\n",
    "# `utils.to_categorical()`: This Keras utility function performs this conversion for us.\n",
    "# - The first argument is the array of integer labels (`y_train` or `y_test`).\n",
    "# - The second argument, `NUM_CLASSES`, tells the function how long the one-hot vector should be\n",
    "#   (i.e., the total number of possible classes).\n",
    "# Practical benefit: This format allows the network's final Softmax layer to directly output\n",
    "# probabilities for each class, which aligns well with loss functions like `categorical_crossentropy`.\n",
    "y_train = utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# In this example, we use a convolutional network (with 2D Convolutional Layers) \n",
    "# to power the neural network.\n",
    "#\n",
    "# Our \"Vanilla\" neural network had a key limitation - we had to flatten all the \n",
    "# pixel data into a single long vector at the very beginning (the Flatten layer).\n",
    "# During that time, we lost information about spatial relationships between pixels,\n",
    "# because the network did not know that pixel (1,2) was next to pixel (1,1), or that\n",
    "# a group of pixels formed a shape.\n",
    "# \n",
    "# Then we passed the pixels into a Dense layer, where every neuron in the current layer is connected\n",
    "# to every neuron in the previous layer. Dense layers are great at learning complex,\n",
    "# non-linear relationships between inputs and outputs. However, because they receive a flattened\n",
    "# input, they essentially treat each pixel as an independent feature.\n",
    "#\n",
    "# CNNs (Convolutional Neural Networks) are specifically designed to address the\n",
    "# limitations of traditional dense networks when dealing with structured data like\n",
    "# images. They exploit the spatial relationships within the data.\n",
    "# \n",
    "# Conv2D layers layers apply \"filters\" (also called kernels) that slide across\n",
    "# the input data (e.g., an image). Each filter is a small matrix of learnable weights.\n",
    "input_layer = layers.Input(shape=(32, 32, 3))\n",
    "\n",
    "# Each Conv2D layer is a core feature extraction layer. \n",
    "#\n",
    "# Take this function call: \n",
    "# layers.Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = \"same\")\n",
    "# \n",
    "# filters = 32: The layer will learn 32 different filters (also called kernels).\n",
    "# Each filter is designed to detect a specific feature (e.g., a vertical edge, a\n",
    "# diagonal line, a specific color blob) in the input image.\n",
    "# \n",
    "# kernel_size = 3: Each filter will be a 3x3 matrix. This means it will look at a\n",
    "# 3x3 patch of the input image at a time.\n",
    "#\n",
    "# strides = 1: The filter will move one pixel at a time across the input image.\n",
    "#\n",
    "# padding = \"same\": This ensures that the output feature map has the same spatial\n",
    "# dimensions (height and width) as the input feature map\n",
    "x = layers.Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = \"same\")(input_layer)\n",
    "# Then, the Batch Normalization layer normalizes the activations of the previous\n",
    "# layer for each batch during training, to remediate gradient explosion. \n",
    "#\n",
    "#  It significantly helps with training stability, speeds up convergence, and can\n",
    "# act as a mild regularizer, reducing the need for extensive dropout (though dropout\n",
    "# is still used later). It makes the network less sensitive to the initialization of weights.\n",
    "x = layers.BatchNormalization()(x)\n",
    "# Finally, we use a LeakyReLU activation function to introduce non-linearity into the\n",
    "# network.\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Then we do the same, with a stride size of 2 (essentially downsampling the spatial\n",
    "# dimensions of the feature maps)\n",
    "x = layers.Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = \"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Then the network learns with 64 different filters, so it's trying to find \n",
    "# more diverse or complex patterns. As we go deeper into a CNN, the filters\n",
    "# tend to learn more abstract and higher-level features by combining the simpler\n",
    "# features detected by earlier layers.\n",
    "x = layers.Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Finally, we learn with 64 different filters again, but downsample with a \n",
    "# stride size of 2, so the spatial dimensions are halved. \n",
    "x = layers.Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = \"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Then we take the multi-dimensional output from the last convolutional block and flatten it into a single 1D vector.\n",
    "# Convolutional layers are excellent at extracting spatial hierarchies of features.\n",
    "# However, for the final classification decision, these extracted features typically\n",
    "# need to be fed into a traditional dense neural network layer, which expects a 1D vector as input.\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128)(x)\n",
    "\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# Dropouts help prevent overfitting. By randomly \"dropping out\" neurons, the network cannot rely\n",
    "# on any single neuron or specific combination of neurons to make predictions. This forces the\n",
    "# network to learn more robust and generalized features, as it needs to be able to make correct\n",
    "# predictions even when some information is missing.\n",
    "x = layers.Dropout(rate = 0.5)(x)\n",
    "\n",
    "output_layer = layers.Dense(10, activation = \"softmax\")(x)\n",
    "\n",
    "model = models.Model(input_layer, output_layer)\n",
    "\n",
    "# Train the model the same way as before\n",
    "opt = optimizers.Adam(learning_rate = 0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size = 32,\n",
    "          epochs = 10,\n",
    "          shuffle = True)\n",
    "\n",
    "model.evaluate(x_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab34a79-15d1-4258-b4c5-b3e06d7fa85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_to_show' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indices):\n\u001b[32m     43\u001b[39m     img = x_test[idx]\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     ax = fig.add_subplot(\u001b[32m1\u001b[39m, \u001b[43mn_to_show\u001b[49m, i + \u001b[32m1\u001b[39m)\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Turn off the axes (ticks and labels) for a cleaner image display.\u001b[39;00m\n\u001b[32m     46\u001b[39m     ax.axis(\u001b[33m'\u001b[39m\u001b[33moff\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'n_to_show' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the human-readable names for each of the 10 CIFAR-10 classes.\n",
    "CLASSES = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "# Use the trained model # to make predictions on the test dataset (x_test).\n",
    "# The `model.predict()` method outputs a probability distribution for each image\n",
    "# over the 10 classes.\n",
    "# For example, for one image, the output might look like [0.01, 0.05, 0.88, ..., 0.02],\n",
    "# where 0.88 is the probability of it being a 'bird'.\n",
    "# This `predictions` array will have a shape of [number_of_test_images, NUM_CLASSES].\n",
    "# For CIFAR-10, x_test has 10,000 images, so `predictions` will be [10000, 10].\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# The `predictions` array contains probabilities (e.g., [0.01, 0.05, 0.88, ...]).\n",
    "# To get the final predicted class for each image, we need to find the class\n",
    "# with the highest probability.\n",
    "#   - `np.argmax()`: This NumPy function returns the *index* of the maximum value along a given axis.\n",
    "#   - `axis = -1`: This specifies that the operation should be performed along the *last* axis.\n",
    "#                  In our `predictions` array (shape [10000, 10]), the last axis is the\n",
    "#                  class probabilities dimension (the 10 possibilities).\n",
    "#                  So, for each of the 10,000 observations, it finds the index (0-9)\n",
    "#                  corresponding to the highest probability.\n",
    "# `preds_single` will be an array of strings like ['bird', 'horse', 'airplane', ...],\n",
    "# representing the model's predicted class for each test image.\n",
    "preds_single = CLASSES[np.argmax(predictions, axis = -1)]\n",
    "# Do the same for the actual (true) labels.\n",
    "actual_single = CLASSES[np.argmax(y_test, axis = -1)]\n",
    "\n",
    "## Visualize the results\n",
    "N_TO_SHOW = 10\n",
    "# Randomly select `n_to_show` indices from the range of available test images.\n",
    "# This ensures we pick a diverse set of images to display.\n",
    "indices = np.random.choice(range(0, len(x_test)), N_TO_SHOW)\n",
    "\n",
    "# Create a figure for plotting.\n",
    "# `figsize = (15, 3)` sets the width of the figure to 15 inches and height to 3 inches.\n",
    "fig = plt.figure(figsize = (15, 3))\n",
    "fig.subplots_adjust(hspace = 0.4, wspace = 0.4)\n",
    "\n",
    "# Loop through the randomly selected indices to display each image and its predictions.\n",
    "for i, idx in enumerate(indices):\n",
    "    img = x_test[idx]\n",
    "    ax = fig.add_subplot(1, N_TO_SHOW, i + 1)\n",
    "    # Turn off the axes (ticks and labels) for a cleaner image display.\n",
    "    ax.axis('off')\n",
    "    # Add text for the predicted label below the image.\n",
    "    ax.text(0.5, -0.35, 'pred = ' + str(preds_single[idx]), fontsize = 10, ha = 'center', transform = ax.transAxes)\n",
    "    ax.text(0.5, -0.7, 'act = ' + str(actual_single[idx]), fontsize = 10, ha = 'center', transform = ax.transAxes)\n",
    "    ax.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2b83d-0897-485e-93e7-06f0a74db1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
