{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ae8ef-051e-43bb-a595-56107801456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "# Load the Fashion MNIST dataset, splitting it into training and testing sets.\n",
    "# x_train, x_test: Grayscale images (originally 28x28 pixels, 0-255 values).\n",
    "# y_train, y_test: Integer labels (0-9).\n",
    "(x_train,y_train),(x_test,y_test) = datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Pre-process the MNIST dataset to be easier to work with.\n",
    "# 1. Normalize pixel values: Convert to float32 and scale from 0-255 to 0.0-1.0.\n",
    "# 2. Pad images: Add 2 pixels of zero-padding around each 28x28 image,\n",
    "#    making them 32x32 pixels. This helps with common CNN input sizes.\n",
    "# 3. Add channel dimension: For grayscale images, add a channel dimension of 1\n",
    "#    (e.g., from (32, 32) to (32, 32, 1)). This is required by Keras Conv2D layers.\n",
    "    def preprocess_mnist_image(images):\n",
    "    images = images.astype(\"float32\") / 255.0\n",
    "    images = np.pad(images, ((0,0), (2,2), (2,2)), constant_values = 0.0)\n",
    "    images = np.expand_dims(images, -1)\n",
    "    return images\n",
    "\n",
    "x_train = preprocess_mnist_image(x_train)\n",
    "x_test = preprocess_mnist_image(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f45214-efc2-469d-94fb-b548a86b9979",
   "metadata": {},
   "source": [
    "## Encoders\n",
    "\n",
    "Encoders can be thought of as feature extractors. They take raw input (in our case, MNIST images) and compress them \n",
    "into a compact but informative representation in a latent space/embedding space (the space of all possible outcomes of outputs). So an image, such as pants with pockets, may be encoded into an embedding within the MNIST dataset's latent space, such as coordinates (5.5, -6.3). This isn't just about shrinking data; it's about making it understandable for downstream tasks by highlighting the key underlying features.\n",
    "\n",
    "## Decoders\n",
    "\n",
    "Decoders are the counterparts to encoders. Given an encoding/embedding/latency representation, they expand it back into an output. Going with the previous example, a decoder may take the (5.5, -6.3) coordinate and turn it back into an image with pants with pockets. \n",
    "\n",
    "## Autoencoders\n",
    "\n",
    "Autoencoders are made up of encoders and decoders. An autoencoder can take an image, encode it into an embedding, and then decode that same embedding into a similar image as the input. In other words, it can take an image, map it to a point in its embedding space (or latent space), and generate some facsimile of the original version. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3c59e-2948-4cc7-a9b7-65aaa412743c",
   "metadata": {},
   "source": [
    "## Encoding: Mapping to a Latent Space\n",
    "\n",
    "To do all that, we will first need to embed images into a latent space using an encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76500c2-c905-401e-a23f-04dccbf107f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = layers.Input(shape=(32,32,1), name=\"encoder_input\"))\n",
    "\n",
    "# Our encoder will progressively extract features and reduce the dimensionality\n",
    "# of the input image, mapping it to a lower-dimensional latent space.\n",
    "# Output shape after this layer: (16, 16, 32)\n",
    "x = layers.Conv2D(32, (3,3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\n",
    "# Output shape after this layer: (8, 8, 64)\n",
    "x = layers.Conv2D(64, (3,3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "# Output shape after this layer: (4, 4, 128)\n",
    "x = layers.Conv2D(128, (3,3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "# Finally, we flatten the 3D output of the last convolutional layer (4x4x128) into \n",
    "# a 1D vector (4 * 4 * 128 = 2048 elements).\n",
    "# This is necessary to connect to a fully connected (Dense) layer. Yes, by flattening,\n",
    "# we DO lose the spatial ifnormation about features that were next to each other in the \n",
    "# 2D maps, because the 2D grid structure gets flattened. \n",
    "# However, we assume this spatial relationship information has already been effectively captured\n",
    "# and encoded by the preceding Conv2D layers.\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Finally, we create a fully-connected output layer. We specify 2 units, as the dimensionality of\n",
    "# the latent space representation. Thus, each input image will be compressed into a 2-dimensional vector.\n",
    "encoder_output = layers.Dense(2, name=\"encoder_output\")(x)\n",
    "\n",
    "encoder = models.Model(encoder_input, encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf7e13-d926-4aa4-a5c7-b484e5074675",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder does the opposite of the encoder - as such, instead of convolutional layers, it uses convolutional tranpose layers. This uses\n",
    "many of the same principles as a standard convolutional layer, instead of downsampling, it is used for upsampling. In other words, given a \n",
    "low-dimension input such as an embedding (e.g. (3,5) in a latent space), the transpose can reconstruct a higher-resolution output (a picture of clothing).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
