{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6ae8ef-051e-43bb-a595-56107801456b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 00:26:24.839515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752207984.990243    4974 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752207985.031197    4974 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752207985.336045    4974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752207985.336078    4974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752207985.336079    4974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752207985.336081    4974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "# Hide TensorFlow verbose logs so they don't pollute the notebook\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, datasets, callbacks\n",
    "\n",
    "# Load the Fashion MNIST dataset, splitting it into training and testing sets.\n",
    "# x_train, x_test: Grayscale images (originally 28x28 pixels, 0-255 values).\n",
    "# y_train, y_test: Integer labels (0-9).\n",
    "(x_train,y_train),(x_test,y_test) = datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Pre-process the MNIST dataset to be easier to work with.\n",
    "# 1. Normalize pixel values: Convert to float32 and scale from 0-255 to 0.0-1.0.\n",
    "# 2. Pad images: Add 2 pixels of zero-padding around each 28x28 image,\n",
    "#    making them 32x32 pixels. This helps with common CNN input sizes.\n",
    "# 3. Add channel dimension: For grayscale images, add a channel dimension of 1\n",
    "#    (e.g., from (32, 32) to (32, 32, 1)). This is required by Keras Conv2D layers.\n",
    "def preprocess_mnist_image(images):\n",
    "    images = images.astype(\"float32\") / 255.0\n",
    "    images = np.pad(images, ((0,0), (2,2), (2,2)), constant_values = 0.0)\n",
    "    images = np.expand_dims(images, -1)\n",
    "    return images\n",
    "\n",
    "x_train = preprocess_mnist_image(x_train)\n",
    "x_test = preprocess_mnist_image(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f45214-efc2-469d-94fb-b548a86b9979",
   "metadata": {},
   "source": [
    "## Encoders\n",
    "\n",
    "Encoders can be thought of as feature extractors. They take raw input (in our case, MNIST images) and compress them \n",
    "into a compact but informative representation in a latent space/embedding space (the space of all possible outcomes of outputs). So an image, such as pants with pockets, may be encoded into an embedding within the MNIST dataset's latent space, such as coordinates (5.5, -6.3). This isn't just about shrinking data; it's about making it understandable for downstream tasks by highlighting the key underlying features.\n",
    "\n",
    "## Decoders\n",
    "\n",
    "Decoders are the counterparts to encoders. Given an encoding/embedding/latency representation, they expand it back into an output. Going with the previous example, a decoder may take the (5.5, -6.3) coordinate and turn it back into an image with pants with pockets. \n",
    "\n",
    "## Autoencoders\n",
    "\n",
    "Autoencoders are made up of encoders and decoders. An autoencoder can take an image, encode it into an embedding, and then decode that same embedding into a similar image as the input. In other words, it can take an image, map it to a point in its embedding space (or latent space), and generate some facsimile of the original version. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3c59e-2948-4cc7-a9b7-65aaa412743c",
   "metadata": {},
   "source": [
    "## Encoding: Mapping to a Latent Space\n",
    "\n",
    "To do all that, we will first need to embed images into a latent space using an encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76500c2-c905-401e-a23f-04dccbf107f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752207989.714806    4974 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21751 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "encoder_input = layers.Input(shape=(32,32,1), name=\"encoder_input\")\n",
    "\n",
    "# Our encoder will progressively extract features and reduce the dimensionality\n",
    "# of the input image, mapping it to a lower-dimensional latent space.\n",
    "# Output shape after this layer: (16, 16, 32)\n",
    "x = layers.Conv2D(32, (3,3), strides=2, activation=\"relu\", padding=\"same\")(encoder_input)\n",
    "# Output shape after this layer: (8, 8, 64)\n",
    "x = layers.Conv2D(64, (3,3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "# Output shape after this layer: (4, 4, 128)\n",
    "x = layers.Conv2D(128, (3,3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "## We'll need this shape later, when we create the decoder.\n",
    "import tensorflow.keras.backend as K\n",
    "shape_before_flattening = K.int_shape(x)[1:]\n",
    "\n",
    "# Finally, we flatten the 3D output of the last convolutional layer (4x4x128) into \n",
    "# a 1D vector (4 * 4 * 128 = 2048 elements).\n",
    "# This is necessary to connect to a fully connected (Dense) layer. Yes, by flattening,\n",
    "# we DO lose the spatial ifnormation about features that were next to each other in the \n",
    "# 2D maps, because the 2D grid structure gets flattened. \n",
    "# However, we assume this spatial relationship information has already been effectively captured\n",
    "# and encoded by the preceding Conv2D layers.\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Finally, we create a fully-connected output layer. We specify 2 units, as the dimensionality of\n",
    "# the latent space representation. Thus, each input image will be compressed into a 2-dimensional vector.\n",
    "encoder_output = layers.Dense(2, name=\"encoder_output\")(x)\n",
    "\n",
    "\n",
    "\n",
    "encoder = models.Model(encoder_input, encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf7e13-d926-4aa4-a5c7-b484e5074675",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder does the opposite of the encoder - as such, instead of convolutional layers, it uses convolutional tranpose layers. This uses\n",
    "many of the same principles as a standard convolutional layer, instead of downsampling, it is used for upsampling. In other words, given a \n",
    "low-dimension input such as an embedding (e.g. (3,5) in a latent space), the transpose can reconstruct a higher-resolution output (a picture of clothing).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c65fe67-8ac0-4a85-aabc-9b32e1b3e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = layers.Input(shape=(2,), name=\"decoder_input\")\n",
    "\n",
    "## Connect the input layer to a dense layer.\n",
    "x = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "x = layers.Reshape(shape_before_flattening)(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(128, (3,3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(64, (3,3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, (3,3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "decoder_output = layers.Conv2D(1, (3,3), strides=1, activation=\"sigmoid\", padding=\"same\", name=\"decoder_output\")(x)\n",
    "\n",
    "decoder = models.Model(decoder_input, decoder_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b344c23c-d05b-44f9-8785-da7a893bd353",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89891d5d-6d5d-4323-8d80-e49838d0ec04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,098</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">246,273</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder_output (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m4,098\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ functional_1 (\u001b[38;5;33mFunctional\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │       \u001b[38;5;34m246,273\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">343,043</span> (1.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m343,043\u001b[0m (1.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">343,043</span> (1.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m343,043\u001b[0m (1.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752207994.735875    5123 service.cc:152] XLA service 0x3dcc3db0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1752207994.735918    5123 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "I0000 00:00:1752207995.158299    5123 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-07-11 00:27:24.249587: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng55{k2=1,k13=0,k14=3,k18=1,k22=0,k23=0} for conv %cudnn-conv.16 = (f32[100,128,4,4]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,128,9,9]{3,2,1,0} %bitcast.3844, f32[128,128,3,3]{3,2,1,0} %bitcast.3835), window={size=3x3 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", metadata={op_type=\"Conv2D\" op_name=\"gradient_tape/functional_2_1/functional_1_1/conv2d_transpose_1/conv_transpose/Conv2D\" source_file=\"/home/brian/Generative-Deep-Learning/.venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-07-11 00:27:24.251646: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 46.267915055s\n",
      "Trying algorithm eng55{k2=1,k13=0,k14=3,k18=1,k22=0,k23=0} for conv %cudnn-conv.16 = (f32[100,128,4,4]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,128,9,9]{3,2,1,0} %bitcast.3844, f32[128,128,3,3]{3,2,1,0} %bitcast.3835), window={size=3x3 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", metadata={op_type=\"Conv2D\" op_name=\"gradient_tape/functional_2_1/functional_1_1/conv2d_transpose_1/conv_transpose/Conv2D\" source_file=\"/home/brian/Generative-Deep-Learning/.venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 35/600\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.6312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752207999.948295    5123 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 0.3609 - val_loss: 0.2606\n",
      "Epoch 2/3\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.2575 - val_loss: 0.2560\n",
      "Epoch 3/3\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.2529 - val_loss: 0.2535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x77f234902000>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = models.Model(\n",
    "    encoder_input, decoder(encoder_output)\n",
    ")\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "autoencoder.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=3,\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test, x_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1187928c-6a69-4aac-a0f5-06d8e5c18d34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Reconstructing content from the embeddings\u001b[39;00m\n\u001b[32m      2\u001b[39m n_to_predict = \u001b[32m50\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m example_images = \u001b[43mx_test\u001b[49m[:n_to_predict]\n\u001b[32m      4\u001b[39m example_labels = y_test[:n_to_predict]\n\u001b[32m      5\u001b[39m predictions = autoencoder.predict(example_images)\n",
      "\u001b[31mNameError\u001b[39m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "## Reconstructing content from the embeddings\n",
    "n_to_predict = 50\n",
    "example_images = x_test[:n_to_predict]\n",
    "example_labels = y_test[:n_to_predict]\n",
    "predictions = autoencoder.predict(example_images)\n",
    "\n",
    "# Borrowed from https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "def display(\n",
    "    images, n=n_to_predict, size=(20, 3), cmap=\"gray_r\", as_type=\"float32\", save_to=None\n",
    "):\n",
    "    plt.figure(figsize=size)\n",
    "    for i in range(n):\n",
    "        _ = plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(images[i].astype(as_type), cmap=cmap)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "display(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44af42-0d2e-4206-a2f8-53fc5ecd36d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
