{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSjyEvY2j9DQpaVJ8/VRaf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Generative Adversarial Networks\n",
        "\n",
        "Generative Adversarial Networks (or GANs) are neural networks that contain two components: The Generator and the Discrinator.\n",
        "\n",
        "First, the GAN receives an original dataset of genuine data.\n",
        "\n",
        "The generator takes in random noise and tries to turn them into observations that look like they were from the original dataset. The discriminator, on the other hand, takes in observations and tries to predict whether they are from the original dataset or if it was a \"forgery\" created by the generator. This create-and-evaluate process is used to create better forgeries from the generator, and likewise for the discriminator to get better at identifying forgeries.\n",
        "\n",
        "In more practical terms:\n",
        "* A original dataset may contain images of cats\n",
        "* The generator is passed in random noise as an input, and tries to turn that noise into cat images (\"forgeries\")\n",
        "* The discriminator tries to determine whether the image was real or forged\n",
        "* The generator and the discriminator both get better at their jobs"
      ],
      "metadata": {
        "id": "ZhXXYLb0xsKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import (\n",
        "    layers,\n",
        "    models,\n",
        "    callbacks,\n",
        "    losses,\n",
        "    utils,\n",
        "    metrics,\n",
        "    optimizers,\n",
        ")\n",
        "\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS = 1\n",
        "BATCH_SIZE = 128\n",
        "Z_DIM = 100\n",
        "EPOCHS = 300\n",
        "LOAD_MODEL = False\n",
        "ADAM_BETA_1 = 0.5\n",
        "ADAM_BETA_2 = 0.999\n",
        "LEARNING_RATE = 0.0002\n",
        "NOISE_PARAM = 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7XEFiLpxyS9",
        "outputId": "004c97d9-267e-435c-fc22-ad46654512cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'lego-brick-images' dataset.\n",
            "Path to dataset files: /kaggle/input/lego-brick-images\n",
            "Found 46384 files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the lego brick images dataset. We're going to make images of bricks.\n",
        "\n",
        "See https://www.kaggle.com/datasets/joosthazelzet/lego-brick-images\n",
        "\n",
        "NOTE: If you're running this in Colab, you'll need to choose one of these options\n",
        "to get the dataset.\n",
        "\n",
        "1. Upload your image data to a directory via the folder icon on the left sidebar.\n",
        "2. Use the Terminal at the bottom of colab to `curl` the dataset into the directory.\n",
        "$ curl -L -o ~/Downloads/lego-brick-images.zip https://www.kaggle.com/api/v1/datasets/download/joosthazelzet/lego-brick-images\n",
        "3. import it via kagglehub library (below)"
      ],
      "metadata": {
        "id": "nQpVwo2A8JLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"joosthazelzet/lego-brick-images\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "train_data = utils.image_dataset_from_directory(\n",
        "    path,\n",
        "    labels=None,\n",
        "    color_mode=\"grayscale\",\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    seed= 0, # The seed parameter in tf.keras.utils.image_dataset_from_directory is used to ensure reproducibility when shuffling the data.\n",
        "    interpolation=\"bilinear\", # The algorithm to use when resizing images\n",
        ")"
      ],
      "metadata": {
        "id": "9JT3e0ac8O6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The images, because they use pixel values, are in the scale of 0 to 255. In order to make them more compatible with activation functions, we need to process them into the range of -1 to 1."
      ],
      "metadata": {
        "id": "3p5H3Wx77zMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "  img = tf.cast(img, tf.float32)\n",
        "  img = (img - 127.5) / 127.5 # The result is in the range [-1, 1] so we can use tanh activation functions on it\n",
        "  return img\n",
        "\n",
        "train_data = train_data.map(lambda x: preprocess(x))"
      ],
      "metadata": {
        "id": "OIvJYe2G8BnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we make the actual discriminator... To understand this, you must understand the **Hierarchy of Features**. A neural network doesn't see a \"face\" or a \"cat\" immediately. It builds that understanding up from the bottom, layer by layer.\n",
        "\n",
        "The seemingly random combinaton of layers has some logic to it that can be\n",
        "explained with this Hierarchy of Features. Below is an example of what this\n",
        "hierarchy can be:\n",
        "\n",
        "### Step 1: The Initial \"Glance\" (Low-Level Features)\n",
        "The model takes the raw pixel input and looks for the most basic building blocks of an image: edges, contrast, and simple lines. e.g. This is the Art Critic glancing at the canvas to check the brushwork. Is the image blurry? Are the lines sharp?\n"
      ],
      "metadata": {
        "id": "ciw1REDI-V5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator_input = layers.Input(shape=(64, 64, 1))\n",
        "\n",
        "x = layers.Conv2D(\n",
        "    64,                  # Learn 64 basic filters (vertical lines, horizontal lines, curves)\n",
        "    kernel_size=4,       # Look at a 4x4 pixel patch at a time\n",
        "    strides=2,           # Move 2 steps over. This shrinks the image size by half (64 -> 32)\n",
        "    padding=\"same\",      # Keep the edges tidy\n",
        "    use_bias=False       # Bias is not strictly necessary here\n",
        ")(discriminator_input)\n",
        "\n",
        "x = layers.LeakyReLU(0.2)(x) # Activation: \"Light up\" the neurons that found a match (e.g., found a line).\n",
        "x = layers.Dropout(0.3)(x)   # Forget 30% of what you saw. Prevents the Critic from memorizing exact pixel values."
      ],
      "metadata": {
        "id": "4o7zJ7JnMI-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Pattern Recognition (Mid-Level Features)\n",
        "\n",
        "The network combines the lines and edges from Step 1 to form textures and shapes. It might find a circle, a corner, or a grid pattern.\n",
        "\n",
        "e.g. The Critic steps back. \"Okay, the lines are sharp. Now, do I see actual shapes? Is that a curve representing an eye, or just random noise?\""
      ],
      "metadata": {
        "id": "nqBbDDYzMdKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input is now 32x32.\n",
        "# We DOUBLE the filters (64 -> 128). Why?\n",
        "# As the image gets smaller physically, the information gets DENSE.\n",
        "# We need more \"drawers\" (filters) to file away these complex combinations of edges.\n",
        "x = layers.Conv2D(\n",
        "    128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
        "\n",
        "x = layers.BatchNormalization()(x) # New addition! Normalizes data to keep training stable.\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "x = layers.Dropout(0.3)(x)"
      ],
      "metadata": {
        "id": "G_V1bJQ2Mn3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Part Detection (High-Level Features)\n",
        "\n",
        "The network combines shapes to find object parts. It looks for specific components relevant to your dataset. For example, if you are training on faces, this layer looks for noses, eyes, and mouths. In our case, we're training on\n",
        "Lego brick images, so we're finding features that make up Lego bricks, like\n",
        "studs and corners. This is where the Discriminator identifies the \"uncanny valley\". A fake brick may look correct in its individual parts, but the overall geometry is subtly wrong or \"impossible.\""
      ],
      "metadata": {
        "id": "dDlAWSYLMonE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input is now 16x16.\n",
        "# Double filters again (128 -> 256).\n",
        "x = layers.Conv2D(\n",
        "    256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
        "\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "x = layers.Dropout(0.3)(x)"
      ],
      "metadata": {
        "id": "j0GSbQ9bM2X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CPUnmmd7P74R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Global Features\n",
        "\n",
        "This is the final, most abstract stage of feature extraction. The network is no longer looking for parts, but verifying the overall quality and plausibility of the complete object. Perhaps we're checking for the overall smoothness and\n",
        "glossiness of a Lego brick."
      ],
      "metadata": {
        "id": "CtotrP6yOfT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "x = layers.Dropout(0.3)(x)"
      ],
      "metadata": {
        "id": "VSmW3IIrOvsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Verdict\n",
        "\n",
        "This final step collapses the vast amount of accumulated information into the single answer the Discriminator must provide. By using a 4x4 kernel on a 4x4 input, we perform a total collapse, resulting in a single 1 x 1 x 1 number.\n",
        "\n",
        "So we output a single number between 0 and 1, where a high value indicates \"Real Lego Brick\" and a low value indicates \"Fake Generator Image.\""
      ],
      "metadata": {
        "id": "uw6Sp9mMPDVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collapsing all 512 features into a single decision score\n",
        "x = layers.Conv2D(1, kernel_size=4, strides=1, padding=\"valid\", use_bias=False)(x)\n",
        "discriminator_output = layers.Flatten()(x)"
      ],
      "metadata": {
        "id": "Q9pZsjHtPVtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the above hierarchy of features may not be the exact and fixed hierarchy that the network learns.' Instaed, we're trying to represent an\n",
        "abstraction that emerges from a learning process, and is supported by\n",
        "technques like visualization (which show that early layers often do detect simple edges). But the *exact* features and their sequences can change slightly from run-to-run due to the randomness of weights and the random nature of the training process.\n",
        "\n",
        "So consider the above hierarchy of features as more of an interpretation."
      ],
      "metadata": {
        "id": "Lo2Ou_UbP8y2"
      }
    }
  ]
}
